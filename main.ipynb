{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization query oriented\n",
    "\n",
    " <hr style=\"border-color:#1d539d\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import copy\n",
    "import gensim\n",
    "import json\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# paths to folders\n",
    "\n",
    "data_json = \"/home/ubuntu/summarization_query_oriented/data/json/patch_0/\"\n",
    "data_txt = \"/home/ubuntu/summarization_query_oriented/data/txt/\"\n",
    "model_folder = \"/home/ubuntu/summarization_query_oriented/models/\"\n",
    "nn_models_folder = \"/home/ubuntu/summarization_query_oriented/nn_models/\"\n",
    "\n",
    "# file names + extensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "# parameters of doc2vec\n",
    "dm = 0\n",
    "min_count = 5\n",
    "window = 10\n",
    "size = 400\n",
    "sample = 1e-4\n",
    "negative = 5\n",
    "workers = 4\n",
    "epoch = 20\n",
    "\n",
    "# Initialize the model ( IMPORTANT )\n",
    "d2v_model = gensim.models.doc2vec.Doc2Vec(dm=dm,min_count=min_count, window=window, size=size, sample=sample, negative=negative, workers=workers,iter = epoch)\n",
    "\n",
    "# load model\n",
    "model_name =\"dm_\"+str(dm)+\"_mc_\"+str(min_count)+\"_w_\"+str(window)+\"_size_\"+str(size)+\"_neg_\"+str(negative)+\"_ep_\"+str(epoch)\n",
    "d2v_model = d2v_model.load(model_folder+model_name+\".d2v\")\n",
    "print(\"model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifiing the model can infer\n",
    "\n",
    "v1 = d2v_model.infer_vector(['my', 'name', 'is', 'charles', 'sutton'])\n",
    "v2 = d2v_model.infer_vector(['my', 'name', 'is', 'charles', 'bitton'])\n",
    "v3 = d2v_model.infer_vector(['my', 'name', 'is', 'charles', 'martin'])\n",
    "\n",
    "np.hstack([v1,v2,v3]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 : fully connected model\n",
    "\n",
    "\n",
    "* __Architecture__ : Fully connected model\n",
    "\n",
    "\n",
    "* __Input__ : a vector that is the concatenation of [ query , partial summary, candidate ]\n",
    "    * *query* : here the subtitle of a wikipedia page\n",
    "    * *partial summary* : here a part (eventually void) of the summary attached to this subtitle\n",
    "    * *candidate* : a random sentence\n",
    "\n",
    "\n",
    "* __Output__ : a score describing how much the candidate sentence is completing the partial summary w.r.t the query \n",
    "\n",
    "\n",
    "* __Training mode__ : we sample triplet from wikipedia data to build the training set, we label 1 if the candidate sentence is a sentence of the correct subsection that is not in the partial summary (by building), we label 0 otherwise\n",
    "\n",
    "\n",
    "* __Testing mode__ : Given a document and a query. The partial summary is initialized as the query, then we choose the sentence of the document that is not in the partial summary with the highest score and delete it from the document. We repeat it until we reach the length limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Processing \n",
    "\n",
    "Here we build functions to perform end—to-end data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "non_selected_keys = [\"title\", \"external links\",\"further reading\",\"references\",\"see also\"]\n",
    "\n",
    "def has_at_least_one_relevant_key(file_as_dict):\n",
    "    \n",
    "    for key in file_as_dict.keys():\n",
    "        b = True\n",
    "        for unwanted_key in non_selected_keys:\n",
    "            if unwanted_key in key.lower() :\n",
    "                b = False    \n",
    "        if b :\n",
    "            return True\n",
    "    return False\n",
    "        \n",
    "def has_irrelevant_content(file_as_dict):\n",
    "    # remove articles with mathematics of chemics\n",
    "    for key in file_as_dict.keys():\n",
    "        if \"{\\\\\" in file_as_dict[key]:\n",
    "            return True        \n",
    "\n",
    "    # check that there is at least one interesting key\n",
    "    if not has_at_least_one_relevant_key(file_as_dict):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def relevant_articles(article_folder_path, min_size = 0.0002) : \n",
    "    \"\"\"\n",
    "    inputs :\n",
    "        - absolute path of the folder containing all the json articles\n",
    "        - min_size : retaining only file with at least size = min_size*10^-4 ko\n",
    "    output : \n",
    "        - article_names: nd array of the names of the relevant articles (absolute paths)\n",
    "        - article_weights : nd array normalized of the weights of each files\n",
    "    \"\"\"\n",
    "    all_names =  [f for f in listdir(article_folder_path)]\n",
    "    article_names = []\n",
    "    article_weights = []\n",
    "    for name in all_names:\n",
    "        article_weight = os.path.getsize(article_folder_path+name)\n",
    "        if article_weight > min_size:\n",
    "            # the size of the article meets the requirement\n",
    "            \n",
    "            with open(article_folder_path+name) as f :\n",
    "                file_as_dict = json.load(f) # get article as dict\n",
    "            \n",
    "            if not has_irrelevant_content(file_as_dict):\n",
    "                article_names.append(article_folder_path+name)\n",
    "                article_weights.append(article_weight)\n",
    "    \n",
    "    article_names = np.asarray(article_names)\n",
    "    article_weights = (np.asarray(article_weights) + 0.0) / np.sum(article_weights)\n",
    "        \n",
    "    return article_names, article_weights\n",
    "            \n",
    "def select_key(file_as_dict):\n",
    "    assert has_at_least_one_relevant_key(file_as_dict), \"the file has no relevant key\"\n",
    "\n",
    "    keys = file_as_dict.keys()\n",
    "    rand_idx = np.random.randint(0,len(keys))\n",
    "    selected_key = keys[rand_idx]\n",
    "    \n",
    "    for unwanted_key in non_selected_keys :\n",
    "        if unwanted_key in selected_key.lower() :\n",
    "            return select_key(file_as_dict)\n",
    "        \n",
    "    return selected_key\n",
    "\n",
    "def create_triplets(d2v_model, article_names, article_weights, nb_triplets=20, triplets_per_file=5, neg_ratio=0.5, str_mode = False) :\n",
    "    \"\"\"\n",
    "    inputs :    \n",
    "        - d2v_model : paragraph vector model \n",
    "        - article_names : ndarray containing the names of the json files (absolute path !)\n",
    "        - article_weights: ndarray normalized of the weight of each files \n",
    "        - nb_triplets : nb of triplets to generate\n",
    "        - triplets_per_file : number of triplet built for each selected file\n",
    "        - neg_ratio : ratio of positives / negative examples. Negative examples are taken inside the article !\n",
    "        \n",
    "    output : \n",
    "        - triplets : nd_array of triplets of shape (nb_triplets+ , embed_dim)\n",
    "        - labels : nd_array of labels of shape (nb_triplets+ ,)\n",
    "\n",
    "    \"\"\"\n",
    "    triplets = []\n",
    "    labels = []\n",
    "    \n",
    "    assert nb_triplets>=triplets_per_file, \"you should have nb_triplets > triplets_per_file\"\n",
    "    \n",
    "    # nb of pos / neg triplets per file\n",
    "    neg_per_file = np.floor(triplets_per_file*neg_ratio) #number of negative triplets to generate given(query + partial summary)\n",
    "    assert neg_per_file >= 1, \"you have to increase your neg_ratio\"\n",
    "    \n",
    "    nb_files = nb_triplets / triplets_per_file\n",
    "    selected_files_array = np.random.choice(article_names, size=nb_files, p=article_weights, replace = False)\n",
    "    \n",
    "    for full_name in selected_files_array :\n",
    "        with open(full_name) as f :\n",
    "            file_as_dict = json.load(f)\n",
    "        \n",
    "        counter = 0\n",
    "        while counter < triplets_per_file :\n",
    "            \n",
    "            # select a key for positive examples\n",
    "            key_pos = select_key(file_as_dict)\n",
    "            \n",
    "            triplet = build_triplet(d2v_model, file_as_dict, key_pos, positive = True, str_mode = str_mode)\n",
    "            label = 1\n",
    "            \n",
    "            triplets.append(triplet)\n",
    "            labels.append(label)\n",
    "            counter += 1 \n",
    "            \n",
    "            if neg_ratio < 1 : \n",
    "                \n",
    "                if np.random.rand() < neg_ratio :\n",
    "                    \n",
    "                    triplet = build_triplet(d2v_model, file_as_dict, key_pos, positive = False, str_mode = str_mode)\n",
    "                    label = 0\n",
    "                    \n",
    "                    triplets.append(triplet)\n",
    "                    labels.append(label)\n",
    "                    counter += 1 \n",
    "\n",
    "            else :\n",
    "                \n",
    "                for n in range(int(np.floor(neg_ratio))):\n",
    "                    \n",
    "                    triplet = build_triplet(d2v_model, file_as_dict, key_pos, positive = False, str_mode = str_mode)\n",
    "                    label = 0\n",
    "                    \n",
    "                    triplets.append(triplet)\n",
    "                    labels.append(label)\n",
    "                    counter += 1 \n",
    "\n",
    "            \n",
    "    triplets = np.asarray(triplets)[:nb_triplets]\n",
    "    labels = np.asarray(labels)[:nb_triplets]\n",
    "    \n",
    "    return triplets, labels\n",
    "\n",
    "def build_triplet(d2v_model, file_as_dict, key_pos, positive = True, str_mode = False):\n",
    "\n",
    "    query_str = key_pos\n",
    "    query_prep = gensim.utils.simple_preprocess(query_str, deacc=True)\n",
    "    query_vector = d2v_model.infer_vector(query_prep)\n",
    "    \n",
    "    summary_str = file_as_dict[key_pos]\n",
    "    sentences = summary_str.split(\".\")\n",
    "    \n",
    "    partial_summary = []\n",
    "    candidates = []\n",
    "    \n",
    "    size_partial_summary = np.random.rand()\n",
    "    \n",
    "    for sentence in sentences: \n",
    "        if np.random.rand() < size_partial_summary :\n",
    "            partial_summary.append(sentence)\n",
    "        else :\n",
    "            candidates.append(sentence)\n",
    "    \n",
    "    candidate = \"\"\n",
    "    counter_candidate = 0\n",
    "    while (candidate == \"\" or partial_summary == \"\") and counter_candidate < 10:\n",
    "        counter_candidate += 1\n",
    "        \n",
    "        if positive : \n",
    "            if len(candidates) > 0:\n",
    "                random_candidate_index = np.random.randint(0,len(candidates))\n",
    "                candidate = candidates[random_candidate_index]\n",
    "            else :\n",
    "                random_candidate_index = np.random.randint(0,len(partial_summary))\n",
    "                candidate = partial_summary[random_candidate_index]\n",
    "                partial_summary[random_candidate_index] = \"\"\n",
    "\n",
    "\n",
    "            candidate_prep = gensim.utils.simple_preprocess(candidate, deacc=True)\n",
    "            candidate_vector = d2v_model.infer_vector(candidate_prep)\n",
    "\n",
    "        else :\n",
    "\n",
    "            key_neg = select_key(file_as_dict)\n",
    "            counter = 0\n",
    "\n",
    "            while key_neg == key_pos and counter<10 : # the counter is for the preproduction code \n",
    "                counter += 1\n",
    "                key_neg = select_key(file_as_dict)\n",
    "\n",
    "            summary_str = file_as_dict[key_neg]\n",
    "\n",
    "            sentences = summary_str.split('.')\n",
    "            random_candidate_index = np.random.randint(0,len(sentences))\n",
    "            candidate = sentences[random_candidate_index]\n",
    "            candidate_prep = gensim.utils.simple_preprocess(candidate, deacc=True)\n",
    "            candidate_vector = d2v_model.infer_vector(candidate_prep)\n",
    "        \n",
    "        partial_summary_str = \"\".join(partial_summary)\n",
    "        partial_summary_prep = gensim.utils.simple_preprocess(partial_summary_str, deacc=True)\n",
    "        partial_summary_vector = d2v_model.infer_vector(partial_summary_prep)\n",
    "    \n",
    "    if str_mode :\n",
    "        return query_str, partial_summary_str, candidate\n",
    "    else :\n",
    "        return np.hstack( [query_vector, partial_summary_vector, candidate_vector] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data processing\n",
    "article_names, article_weights = relevant_articles(data_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here you can play with the triplet maker and see what gives triplet labelisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "label =  1\n",
      "\n",
      "query : Georg Wilhelm Friedrich Hegel Legacy Left and Right Hegelianism\n",
      "\n",
      "partial summary : Some historians have spoken of Hegel's influence as represented by two opposing camps No Hegelians of the period ever referred to themselves as \"Right Hegelians\"; that was a term of insult originated by David Strauss, a self-styled Left Hegelian The Italian Fascist Giovanni Gentile, according to Benedetto Croce, \" Walter Jaeschke and Otto Pöggeler in Germany, as well as Peter Hodgson and Howard Kainz in America are notable for their recent contributions to post-USSR thinking about Hegel\n",
      "\n",
      "candidate : \n",
      "The Left Hegelians also spawned Marxism, which inspired global movements, encompassing the Russian Revolution, the Chinese Revolution, and myriad revolutionary practices up until the present moment\n",
      "--------------------------------------------------\n",
      "label =  0\n",
      "\n",
      "query : Georg Wilhelm Friedrich Hegel Legacy Left and Right Hegelianism\n",
      "\n",
      "partial summary : Some historians have spoken of Hegel's influence as represented by two opposing camps The Left Hegelians, also known as the Young Hegelians, interpreted Hegel in a revolutionary sense, leading to an advocation of atheism in religion and liberal democracy in politics\n",
      "In more recent studies, however, this paradigm has been questioned No Hegelians of the period ever referred to themselves as \"Right Hegelians\"; that was a term of insult originated by David Strauss, a self-styled Left Hegelian Critiques of Hegel offered from the Left Hegelians radically diverted Hegel's thinking into new directions and eventually came to form a disproportionately large part of the literature on and about Hegel\n",
      "The Left Hegelians also spawned Marxism, which inspired global movements, encompassing the Russian Revolution, the Chinese Revolution, and myriad revolutionary practices up until the present moment\n",
      "Twentieth-century interpretations of Hegel were mostly shaped by British Idealism, logical positivism, Marxism, and Fascism The Italian Fascist Giovanni Gentile, according to Benedetto Croce, \" holds the honor of having been the most rigorous neo-Hegelian in the entire history of Western philosophy and the dishonor of having been the official philosopher of Fascism in Italy\" However, since the fall of the USSR, a new wave of Hegel scholarship arose in the West, without the preconceptions of the prior schools of thought Walter Jaeschke and Otto Pöggeler in Germany, as well as Peter Hodgson and Howard Kainz in America are notable for their recent contributions to post-USSR thinking about Hegel\n",
      "\n",
      "candidate :  Popper further proposed that Hegel's philosophy served not only as an inspiration for communist and fascist totalitarian governments of the 20th century, whose dialectics allow for any belief to be construed as rational simply if it could be said to exist\n",
      "--------------------------------------------------\n",
      "label =  1\n",
      "\n",
      "query : Georg Wilhelm Friedrich Hegel Works\n",
      "\n",
      "partial summary : \n",
      "\n",
      "candidate :  He also published some articles early in his career and during his Berlin period\n",
      "--------------------------------------------------\n",
      "label =  0\n",
      "\n",
      "query : Georg Wilhelm Friedrich Hegel Works\n",
      "\n",
      "partial summary :  In his political philosophy, he criticized Karl Ludwig von Haller's reactionary work, which claimed that laws were not necessary A number of other works on the philosophy of history, religion, aesthetics, and the history of philosophy were compiled from the lecture notes of his students and published posthumously\n",
      "\n",
      "candidate : \" In 1820, Schopenhauer became a lecturer at the University of Berlin, and he scheduled his lectures to coincide with those of Hegel, whom Schopenhauer had also described as a \"clumsy charlatan\"\n",
      "--------------------------------------------------\n",
      "label =  1\n",
      "\n",
      "query : Georg Wilhelm Friedrich Hegel Legacy Triads\n",
      "\n",
      "partial summary :  the French Revolution) would cause the creation of its \"antithesis\" (eg the constitutional state of free citizens) However, Hegel used this classification only once, and he attributed the terminology to Kant The terminology was largely developed earlier by Fichte being–nothingness–becoming, immediate–mediate–concrete, abstract–negative–concrete) is about this movement from inner contradiction to higher-level integration or unification Believing that the traditional description of Hegel's philosophy in terms of thesis–antithesis–synthesis was mistaken, a few scholars, like Raya Dunayevskaya, have attempted to discard the triadic approach altogethere the subjective side of knowledge and will, with its life, movement, and activity\" (thesis and antithesis) he doesn't use \"synthesis\" but instead speaks of the \"Whole\": \"We then recognised the State as the moral Whole and the Reality of Freedom, and consequently as the objective unity of these two elements Nevertheless, such is the persistence of this misnomer that the model and terminology survive in a number of scholarly works\n",
      "\n",
      "candidate :  From Hegel's point of view, analysis or comprehension of a thing or idea reveals that underneath its apparently simple identity or unity is an underlying inner contradiction\n",
      "--------------------------------------------------\n",
      "label =  0\n",
      "\n",
      "query : Georg Wilhelm Friedrich Hegel Legacy Triads\n",
      "\n",
      "partial summary : \n",
      "It is widely admitted today that the old-fashioned description of Hegel's philosophy in terms of thesis–antithesis–synthesis is inaccurate\n",
      "\n",
      "candidate :  Of special importance is his concept of spirit (Geist: sometimes also translated as \"mind\") as the historical manifestation of the logical concept and the \"sublation\" (Aufhebung: integration without elimination or reduction) of seemingly contradictory or opposing factors; examples include the apparent opposition between nature and freedom and between immanence and transcendence\n",
      "--------------------------------------------------\n",
      "label =  1\n",
      "\n",
      "query : Georg Wilhelm Friedrich Hegel Thought Civil society\n",
      "\n",
      "partial summary : Hegel made the distinction between civil society and state in his Elements of the Philosophy of Right Broadly speaking, the term was split, like Hegel's followers, to the political left and right On the left, it became the foundation for Karl Marx's civil society as an economic base; to the right, it became a description for all non-state (and the state is the peak of the objective spirit) aspects of society, including culture, society and politics Thus, it was perfectly legitimate in the eyes of Hegel for a conqueror, such as Napoleon, to come along and destroy that which was not fully realized\n",
      "\n",
      "candidate :  This liberal distinction between political society and civil society was followed by Alexis de Tocqueville\n",
      "--------------------------------------------------\n",
      "label =  0\n",
      "\n",
      "query : Georg Wilhelm Friedrich Hegel Thought Civil society\n",
      "\n",
      "partial summary :  Broadly speaking, the term was split, like Hegel's followers, to the political left and right On the left, it became the foundation for Karl Marx's civil society as an economic base; to the right, it became a description for all non-state (and the state is the peak of the objective spirit) aspects of society, including culture, society and politics\n",
      "\n",
      "candidate : \n",
      "The Left Hegelians also spawned Marxism, which inspired global movements, encompassing the Russian Revolution, the Chinese Revolution, and myriad revolutionary practices up until the present moment\n",
      "--------------------------------------------------\n",
      "label =  1\n",
      "\n",
      "query : Bharatiya Janata Party campaign for Indian general election, 2014 I Support Narendra Modi\n",
      "\n",
      "partial summary : \n",
      "\n",
      "candidate :  ISN used social media tool to mobilize youth during the election  and supported initiatives such as blood donation camps along with political activism\n",
      "--------------------------------------------------\n",
      "label =  0\n",
      "\n",
      "query : Bharatiya Janata Party campaign for Indian general election, 2014 I Support Narendra Modi\n",
      "\n",
      "partial summary : \n",
      "\n",
      "candidate :  It said no to FDI in multi-brand retail but said that FDI will be allowed in sectors wherever needed for job and asset creation, infrastructure and acquisition of niche technology and specialised expertise\n",
      "--------------------------------------------------\n",
      "label =  1\n",
      "\n",
      "query : Bharatiya Janata Party campaign for Indian general election, 2014 Chai Pe Charcha\n",
      "\n",
      "partial summary :  In this campaign, the BJP's prime ministerial candidate for 2014 General Elections, Narendra Modi, interacts with people at a tea stall in the predetermined places using a combination of satellite, DTH, internet and mobile\n",
      "List of events\n",
      "\n",
      "candidate : Chai Pe Charcha (Hindi phrase for discussion over tea) is an innovative campaign organised by the BJP along with the Citizens for Accountable Governance, a political advocacy group founded by election strategist Prashant Kishor\n",
      "--------------------------------------------------\n",
      "label =  0\n",
      "\n",
      "query : Bharatiya Janata Party campaign for Indian general election, 2014 Chai Pe Charcha\n",
      "\n",
      "partial summary :  In this campaign, the BJP's prime ministerial candidate for 2014 General Elections, Narendra Modi, interacts with people at a tea stall in the predetermined places using a combination of satellite, DTH, internet and mobile\n",
      "List of events\n",
      "\n",
      "candidate : \"I Support Narendra Modi\" (ISN) was an Indian social advocacy group advocating Narendra Modi as Prime Minister of India\n",
      "--------------------------------------------------\n",
      "label =  1\n",
      "\n",
      "query : Bharatiya Janata Party campaign for Indian general election, 2014 Issues Inflation\n",
      "\n",
      "partial summary : 55% as of August 2012, the highest amotrade (counting exports and imports) stands at $6067 billion and is currently the 9th largest in the world\n",
      "\n",
      "candidate : Inflation remains stubbornly high at 7\n",
      "--------------------------------------------------\n",
      "label =  0\n",
      "\n",
      "query : Bharatiya Janata Party campaign for Indian general election, 2014 Issues Inflation\n",
      "\n",
      "partial summary : \n",
      "\n",
      "candidate : The Bharatiya Janata Party (BJP) is one of the two major political parties in India and was the main opposition party during the 15th Lok Sabha\n",
      "--------------------------------------------------\n",
      "label =  1\n",
      "\n",
      "query : Bharatiya Janata Party campaign for Indian general election, 2014 Background\n",
      "\n",
      "partial summary : The 15th Lok Sabha was due to complete its constitutional term on 31 May 2014 Hence the general election was declared by the Election Commission for the constitution of 16th Lok Sabha in India The election were held in nine phases from 7 April to 12 May 2014\n",
      "\n",
      "candidate :  Following its consecutive defeat in the 2004 and 2009 general elections, BJP had been the principal opposition party in parliament and claimed to secure largest number of parliamentary seats under the leadership of its prime ministerial candidate Narendra Modi who had been gaining ground for a national role after his continued term of 14 years as Gujarat Chief Minister\n",
      "--------------------------------------------------\n",
      "label =  0\n",
      "\n",
      "query : Bharatiya Janata Party campaign for Indian general election, 2014 Background\n",
      "\n",
      "partial summary : \n",
      "\n",
      "candidate :  K\n",
      "--------------------------------------------------\n",
      "label =  1\n",
      "\n",
      "query : Greek diacritics Computer encoding Unicode\n",
      "\n",
      "partial summary :  For example, the monotonic \"Greek small letter alpha with tónos\" is at U+03AC, while the polytonic \"Greek small letter alpha with oxeîa\" is at U+1F71\n",
      "Below are the accented characters provided in Unicode\n",
      "\n",
      "candidate : While the tónos of monotonic orthography looks similar to the oxeîa of polytonic orthography in most fonts, Unicode has historically separate symbols for letters with these diacritics\n",
      "--------------------------------------------------\n",
      "label =  0\n",
      "\n",
      "query : Greek diacritics Computer encoding Unicode\n",
      "\n",
      "partial summary :  The monotonic and polytonic accent however have been de jure equivalent since 1986, and accordingly the oxeîa diacritic in Unicode decomposes canonically to the monotonic tónos — both are underlyingly treated as equivalent to the Latin acute accent, U+0301\n",
      "Below are the accented characters provided in Unicode\n",
      "\n",
      "candidate :  The diaeresis is always written\n",
      "--------------------------------------------------\n",
      "label =  1\n",
      "\n",
      "query : Greek diacritics History Stress accent\n",
      "\n",
      "partial summary : \n",
      "\n",
      "candidate : In the later development of the language, the ancient pitch accent was replaced by an intensity or stress accent, making the three types of accent identical, and the /h/ sound became silent\n",
      "--------------------------------------------------\n",
      "label =  0\n",
      "\n",
      "query : Greek diacritics History Stress accent\n",
      "\n",
      "partial summary : In the later development of the language, the ancient pitch accent was replaced by an intensity or stress accent, making the three types of accent identical, and the /h/ sound became silent\n",
      "\n",
      "candidate :  The Greek alphabet is attested since the 8th century BC\n",
      "--------------------------------------------------\n",
      "label =  1\n",
      "\n",
      "query : Greek diacritics History Official adoption of monotonic system\n",
      "\n",
      "partial summary :  This simplification has been criticized on the grounds that polytonic orthography provides a cultural link to the past\n",
      "\n",
      "candidate : Following the official adoption of the Demotic form of the language, the monotonic orthography was imposed by law in 1982\n",
      "--------------------------------------------------\n",
      "label =  0\n",
      "\n",
      "query : Greek diacritics History Official adoption of monotonic system\n",
      "\n",
      "partial summary : Following the official adoption of the Demotic form of the language, the monotonic orthography was imposed by law in 1982 The latter uses only the acute accent (or sometimes a vertical bar intentionally distinct from any of the traditional accents) and diaeresis and omits the breathings This simplification has been criticized on the grounds that polytonic orthography provides a cultural link to the past\n",
      "\n",
      "candidate : \n",
      "The grave accent (Ancient Greek: βαρεῖα bareîa \"heavy\" or \"low\") — ὰ — marked normal or low pitch\n",
      "--------------------------------------------------\n",
      "label =  1\n",
      "\n",
      "query : Greek diacritics Description Coronis\n",
      "\n",
      "partial summary : The coronis (Ancient Greek: κορωνίς korōnís \"curved\") marks a vowel contracted by crasis\n",
      "\n",
      "candidate :  It was formerly an apostrophe placed after the contracted vowel, but is now placed over the vowel and is identical to the smooth breathing\n",
      "--------------------------------------------------\n",
      "label =  0\n",
      "\n",
      "query : Greek diacritics Description Coronis\n",
      "\n",
      "partial summary : \n",
      "\n",
      "candidate :  It retains two diacritics: a single accent or tonos ( ΄ ), which indicates stress and the diaeresis ( ¨ ), which usually indicates a hiatus but occasionally indicates a diphthong: compare modern Greek παϊδάκια (/pajˈðaca/, \"lamb chops\"), with a diphthong, and παιδάκια (/peˈðaca/, \"little children\") with a simple vowel\n"
     ]
    }
   ],
   "source": [
    "triplets, labels = create_triplets(d2v_model, article_names, article_weights, nb_triplets=25, triplets_per_file=8, neg_ratio=1, str_mode = True)\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    print 50*'-'\n",
    "    print \"label = \", labels[i]\n",
    "    print \"\\nquery :\", triplets[i][0]\n",
    "    print \"\\npartial summary :\", triplets[i][1]\n",
    "    print \"\\ncandidate :\", triplets[i][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "fc_model_name = nn_models_folder + time.strftime(\"%Y_%m_%d_\") +'_fc_model.h5' \n",
    "\n",
    "fc_model = Sequential()\n",
    "\n",
    "fc_model.add(Dense(120, input_dim=1200))\n",
    "fc_model.add(Activation('sigmoid'))\n",
    "fc_model.add(Dropout(0.5))\n",
    "\n",
    "fc_model.add(Dense(12))\n",
    "fc_model.add(Activation('sigmoid'))\n",
    "fc_model.add(Dropout(0.5))\n",
    "\n",
    "fc_model.add(Dense(1))\n",
    "fc_model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "fc_model.compile(loss=\"binary_crossentropy\", optimizer=adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training (we use training per batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "for i in range(200):\n",
    "    if i%10 == 0 : \n",
    "        print(i)\n",
    "    triplets, labels = create_triplets(d2v_model, article_names, article_weights, nb_triplets=batch_size, triplets_per_file=16, neg_ratio=1, str_mode = False)\n",
    "    fc_model.train_on_batch(triplets, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 0s\n"
     ]
    }
   ],
   "source": [
    "triplets_tests , labels_tests = create_triplets(d2v_model, article_names, article_weights, nb_triplets=128, triplets_per_file=16, neg_ratio=1, str_mode = False)\n",
    "labels_predicted = fc_model.predict(triplets_tests , batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200,)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplets_tests[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 0s\n",
      "0.693373680115\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = fc_model.evaluate(triplets_tests, labels_tests, batch_size=batch_size)\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "fc_model.save(nn_models_folder + 'fc_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "del fc_model  # deletes the existing model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "fc_model = load_model(nn_models_folder +'fc_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General info on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_9 (Dense)                  (None, 120)           144120      dense_input_8[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 120)           0           dense_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 120)           0           activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 12)            1452        dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 12)            0           dense_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 12)            0           activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 1)             13          dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 1)             0           dense_11[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 145585\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'class_name': 'Dense',\n",
       "  'config': {'W_constraint': None,\n",
       "   'W_regularizer': None,\n",
       "   'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'b_constraint': None,\n",
       "   'b_regularizer': None,\n",
       "   'batch_input_shape': (None, 1200),\n",
       "   'bias': True,\n",
       "   'init': 'glorot_uniform',\n",
       "   'input_dim': 1200,\n",
       "   'input_dtype': u'float32',\n",
       "   'name': u'dense_9',\n",
       "   'output_dim': 120,\n",
       "   'trainable': True}},\n",
       " {'class_name': 'Activation',\n",
       "  'config': {'activation': 'sigmoid',\n",
       "   'name': u'activation_9',\n",
       "   'trainable': True}},\n",
       " {'class_name': 'Dropout',\n",
       "  'config': {'name': u'dropout_3', 'p': 0.5, 'trainable': True}},\n",
       " {'class_name': 'Dense',\n",
       "  'config': {'W_constraint': None,\n",
       "   'W_regularizer': None,\n",
       "   'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'b_constraint': None,\n",
       "   'b_regularizer': None,\n",
       "   'bias': True,\n",
       "   'init': 'glorot_uniform',\n",
       "   'input_dim': None,\n",
       "   'name': u'dense_10',\n",
       "   'output_dim': 12,\n",
       "   'trainable': True}},\n",
       " {'class_name': 'Activation',\n",
       "  'config': {'activation': 'sigmoid',\n",
       "   'name': u'activation_10',\n",
       "   'trainable': True}},\n",
       " {'class_name': 'Dropout',\n",
       "  'config': {'name': u'dropout_4', 'p': 0.5, 'trainable': True}},\n",
       " {'class_name': 'Dense',\n",
       "  'config': {'W_constraint': None,\n",
       "   'W_regularizer': None,\n",
       "   'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'b_constraint': None,\n",
       "   'b_regularizer': None,\n",
       "   'bias': True,\n",
       "   'init': 'glorot_uniform',\n",
       "   'input_dim': None,\n",
       "   'name': u'dense_11',\n",
       "   'output_dim': 1,\n",
       "   'trainable': True}},\n",
       " {'class_name': 'Activation',\n",
       "  'config': {'activation': 'sigmoid',\n",
       "   'name': u'activation_11',\n",
       "   'trainable': True}}]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def summarize(text, query, d2v_model, nn_model, limit = 2000):\n",
    "\n",
    "    query_prep = gensim.utils.simple_preprocess(query, deacc=True)\n",
    "    query_vector = d2v_model.infer_vector(query_prep)\n",
    "    \n",
    "    summary  = \"\"\n",
    "    summary_vector = d2v_model.infer_vector([\"\"])\n",
    "    summary_idx = []\n",
    "    \n",
    "    sentences = text.split('.')\n",
    "    sentences = np.asarray(sentences)\n",
    "    \n",
    "    remaining_sentences = copy.copy(sentences)\n",
    "    \n",
    "    size = 0\n",
    "    \n",
    "    while size < limit and len(remaining_sentences)>0 :\n",
    "        \n",
    "        for sentence in remaining_sentences :\n",
    "            scores = []\n",
    "            sentence_prep = gensim.utils.simple_preprocess(sentence, deacc=True)\n",
    "            sentence_vector = d2v_model.infer_vector(sentence_prep)\n",
    "            print query_vector.shape, summary_vector.shape, sentence_vector.shape\n",
    "            nn_input = np.hstack([query_vector, summary_vector, sentence_vector])\n",
    "            \n",
    "            scores.append(nn_model.predict(nn_input, batch_size=1))\n",
    "            \n",
    "            max_idx_rem = np.argmax(scores)\n",
    "            \n",
    "            idx_selected_sentence = np.arange(len(sentences))[sentences == remaining_sentences[max_idx_rem]]\n",
    "            \n",
    "            size += len(remaining_sentences[max_idx_rem])\n",
    "\n",
    "            del remaining_sentences[max_idx_rem]\n",
    "            \n",
    "            summary_idx.append(idx_selected_sentence)\n",
    "            summary_idx.sort()\n",
    "            \n",
    "            summary  = \"\"\n",
    "            for idx in summary_idx:\n",
    "                summary = summary + \" \" + sentences[summary_idx]\n",
    "            \n",
    "            summary_prep = gensim.utils.simple_preprocess(summary, deacc=True)\n",
    "            summary_vector = d2v_model.infer_vector(summary_prep)\n",
    "            \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "query History of Israel State of Israel (1948–present) 1974–1977: Rabin I\n",
      "**************************************************\n",
      "real summary\n",
      "\n",
      "Following Meir's resignation, Yitzhak Rabin (Chief of Staff during the Six Day War) became prime minister. Modern Orthodox Jews (Religious Zionist followers of the teachings of Rabbi Kook), formed the Gush Emunim movement, and began an organized drive to settle the West Bank and Gaza Strip. In November 1975 the United Nations General Assembly, under the guidance of Austrian Secretary General Kurt Waldheim, adopted Resolution 3379, which asserted Zionism to be a form of racism. The General Assembly rescinded this resolution in December 1991 with Resolution 46/86. In March 1976 there was a massive strike by Israeli-Arabs in protest at a government plan to expropriate land in the Galilee.\n",
      "In July 1976, an Air France plane carrying 260 people was hijacked by Palestinian and German terrorists and flown to Uganda, then ruled by Idi Amin Dada. There, the Germans separated the Jewish passengers from the non-Jewish passengers, releasing the non-Jews. The hijackers threatened to kill the remaining, 100-odd Jewish passengers (and the French crew who had refused to leave). Despite the distances involved, Rabin ordered a daring rescue operation in which the kidnapped Jews were freed. UN Secretary General Waldheim described the raid as \"a serious violation of the national sovereignty of a United Nations member state\" (meaning Uganda). Waldheim was a former Nazi and suspected war criminal, with a record of offending Jewish sensibilities.\n",
      "In 1976, the ongoing Lebanese Civil War led Israel to allow South Lebanese to cross the border and work in Israel. In January 1977, French authorities arrested Abu Daoud, the planner of the Munich massacre, releasing him a few days later. In March 1977 Anatoly Sharansky, a prominent Refusenik and spokesman for the Moscow Helsinki Group, was sentenced to 13 years' hard labour.\n",
      "Rabin resigned on April 1977 after it emerged that his wife maintained a dollar account in the United States (illegal at the time), which had been opened while Rabin was Israeli ambassador. The incident became known as the Dollar Account affair. Shimon Peres informally replaced him as prime minister, leading the Alignment in the subsequent elections.\n",
      "**************************************************\n",
      "nn summary\n",
      "\n",
      "(400,) (400,) (400,)\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Error when checking : expected dense_input_9 to have shape (None, 1200) but got array with shape (1200, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-bbfa5022fe2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"real summary\\n\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummary_true\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;34m\"*\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"nn summary\\n\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md2v_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfc_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlimit_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-94-d9d0f952da79>\u001b[0m in \u001b[0;36msummarize\u001b[1;34m(text, query, d2v_model, nn_model, limit)\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mnn_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mquery_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummary_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence_vector\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mmax_idx_rem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[0;32m    646\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 648\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[0;32m   1160\u001b[0m         x = standardize_input_data(x, self.input_names,\n\u001b[0;32m   1161\u001b[0m                                    \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minternal_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1162\u001b[1;33m                                    check_batch_dim=False)\n\u001b[0m\u001b[0;32m   1163\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1164\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_dim, exception_prefix)\u001b[0m\n\u001b[0;32m    106\u001b[0m                                         \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m                                         \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m                                         str(array.shape))\n\u001b[0m\u001b[0;32m    109\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Error when checking : expected dense_input_9 to have shape (None, 1200) but got array with shape (1200, 1)"
     ]
    }
   ],
   "source": [
    "wikipedia_title = \"History of Israel\"\n",
    "with open(data_json+wikipedia_title+\".json\", 'r') as f:\n",
    "    wiki_as_json = json.load(f)\n",
    "\n",
    "text = \"\"\n",
    "for key in wiki_as_json.keys():\n",
    "    if key not in non_selected_keys:\n",
    "        text += \" \" + wiki_as_json[key]\n",
    "        \n",
    "random_idx = np.random.randint(0,len(wiki_as_json.keys()))\n",
    "query = wiki_as_json.keys()[random_idx]\n",
    "summary_true = wiki_as_json[query]\n",
    "limit_size = len(wiki_as_json[query])\n",
    "\n",
    "print 50*\"*\"\n",
    "print 'query', query\n",
    "print 50*\"*\"\n",
    "print \"real summary\\n\\n\", summary_true\n",
    "print 50*\"*\"\n",
    "print \"nn summary\\n\\n\", summarize(text,query,d2v_model,fc_model, limit = limit_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'class_name': 'Dense',\n",
       "  'config': {'W_constraint': None,\n",
       "   'W_regularizer': None,\n",
       "   'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'b_constraint': None,\n",
       "   'b_regularizer': None,\n",
       "   'batch_input_shape': (None, 1200),\n",
       "   'bias': True,\n",
       "   'init': 'glorot_uniform',\n",
       "   'input_dim': 1200,\n",
       "   'input_dtype': u'float32',\n",
       "   'name': u'dense_9',\n",
       "   'output_dim': 120,\n",
       "   'trainable': True}},\n",
       " {'class_name': 'Activation',\n",
       "  'config': {'activation': 'sigmoid',\n",
       "   'name': u'activation_9',\n",
       "   'trainable': True}},\n",
       " {'class_name': 'Dropout',\n",
       "  'config': {'name': u'dropout_3', 'p': 0.5, 'trainable': True}},\n",
       " {'class_name': 'Dense',\n",
       "  'config': {'W_constraint': None,\n",
       "   'W_regularizer': None,\n",
       "   'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'b_constraint': None,\n",
       "   'b_regularizer': None,\n",
       "   'bias': True,\n",
       "   'init': 'glorot_uniform',\n",
       "   'input_dim': None,\n",
       "   'name': u'dense_10',\n",
       "   'output_dim': 12,\n",
       "   'trainable': True}},\n",
       " {'class_name': 'Activation',\n",
       "  'config': {'activation': 'sigmoid',\n",
       "   'name': u'activation_10',\n",
       "   'trainable': True}},\n",
       " {'class_name': 'Dropout',\n",
       "  'config': {'name': u'dropout_4', 'p': 0.5, 'trainable': True}},\n",
       " {'class_name': 'Dense',\n",
       "  'config': {'W_constraint': None,\n",
       "   'W_regularizer': None,\n",
       "   'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'b_constraint': None,\n",
       "   'b_regularizer': None,\n",
       "   'bias': True,\n",
       "   'init': 'glorot_uniform',\n",
       "   'input_dim': None,\n",
       "   'name': u'dense_11',\n",
       "   'output_dim': 1,\n",
       "   'trainable': True}},\n",
       " {'class_name': 'Activation',\n",
       "  'config': {'activation': 'sigmoid',\n",
       "   'name': u'activation_11',\n",
       "   'trainable': True}}]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 : LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4]\n",
      "[1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "list1 = [1,2,3,4]\n",
    "list2 = copy.copy(list1)\n",
    "del list1[0]\n",
    "print (list1)\n",
    "print (list2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
