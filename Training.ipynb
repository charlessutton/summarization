{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "# training file\n",
    "\n",
    "# In this script we perform the training of the fully connected model\n",
    "\n",
    "# Import \n",
    "import bisect\n",
    "import collections\n",
    "import copy\n",
    "import gensim\n",
    "import json\n",
    "import keras\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import pyrouge\n",
    "from pyrouge import Rouge155\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# paths to folder \n",
    "data_json = \"/home/ubuntu/summarization_query_oriented/data/json/patch_0/\"\n",
    "data_txt = \"/home/ubuntu/summarization_query_oriented/data/txt/\"\n",
    "model_folder = \"/home/ubuntu/summarization_query_oriented/models/\"\n",
    "nn_models_folder = \"/home/ubuntu/summarization_query_oriented/nn_models/\"\n",
    "title_file = \"/home/ubuntu/summarization_query_oriented/DUC/duc2005_topics.sgml\"\n",
    "titles_folder = \"/home/ubuntu/summarization_query_oriented/DUC/duc2005_docs/\"\n",
    "\n",
    "# training parameters\n",
    "\n",
    "patience_limit = 3\n",
    "\n",
    "# validation data \n",
    "\n",
    "X_val = np.load(\"/home/ubuntu/summarization_query_oriented/valset/X_val.npy\")\n",
    "y_val = np.load(\"/home/ubuntu/summarization_query_oriented/valset/y_val.npy\")\n",
    "\n",
    "\n",
    "# useful functions to put in a separate file next\n",
    "\n",
    "non_selected_keys = [\"title\", \"external links\",\"further reading\",\"references\",\"see also\"]\n",
    "\n",
    "def has_at_least_one_relevant_key(file_as_dict):\n",
    "    \n",
    "    for key in file_as_dict.keys():\n",
    "        b = True\n",
    "        for unwanted_key in non_selected_keys:\n",
    "            if unwanted_key in key.lower() :\n",
    "                b = False    \n",
    "        if b :\n",
    "            return True\n",
    "    return False\n",
    "        \n",
    "def has_irrelevant_content(file_as_dict):\n",
    "    # remove articles with mathematics of chemics\n",
    "    for key in file_as_dict.keys():\n",
    "        if \"{\\\\\" in file_as_dict[key]:\n",
    "            return True        \n",
    "\n",
    "    # check that there is at least one interesting key\n",
    "    if not has_at_least_one_relevant_key(file_as_dict):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def relevant_articles(article_folder_path, min_size = 10000) : \n",
    "    \"\"\"\n",
    "    inputs :\n",
    "        - absolute path of the folder containing all the json articles\n",
    "        - min_size : retaining only file with at least size = min_size*10^-4 ko\n",
    "    output : \n",
    "        - article_names: nd array of the names of the relevant articles (absolute paths)\n",
    "        - article_weights : nd array normalized of the weights of each files\n",
    "    \"\"\"\n",
    "    all_names =  [f for f in listdir(article_folder_path)]\n",
    "    article_names = []\n",
    "    article_weights = []\n",
    "    for name in all_names:\n",
    "        article_weight = os.path.getsize(article_folder_path+name)\n",
    "        if article_weight > min_size:\n",
    "            # the size of the article meets the requirement\n",
    "            \n",
    "            with open(article_folder_path+name) as f :\n",
    "                file_as_dict = json.load(f) # get article as dict\n",
    "            \n",
    "            if not has_irrelevant_content(file_as_dict):\n",
    "                article_names.append(article_folder_path+name)\n",
    "                article_weights.append(article_weight)\n",
    "    \n",
    "    article_names = np.asarray(article_names)\n",
    "    article_weights = (np.asarray(article_weights) + 0.0) / np.sum(article_weights)\n",
    "        \n",
    "    return article_names, article_weights\n",
    "            \n",
    "def select_key(file_as_dict, patience = 10):\n",
    "    if patience > 0 :\n",
    "        assert has_at_least_one_relevant_key(file_as_dict), \"the file has no relevant key\"\n",
    "\n",
    "        keys = file_as_dict.keys()\n",
    "        rand_idx = np.random.randint(0,len(keys))\n",
    "        selected_key = keys[rand_idx]\n",
    "\n",
    "        if len(file_as_dict[selected_key].split(\".\"))<=2:\n",
    "            return select_key(file_as_dict, patience = patience - 1)\n",
    "\n",
    "        for unwanted_key in non_selected_keys :\n",
    "            if unwanted_key in selected_key.lower() :\n",
    "                return select_key(file_as_dict, patience = patience - 1)\n",
    "\n",
    "        return selected_key\n",
    "    else : \n",
    "        keys = file_as_dict.keys()\n",
    "        rand_idx = np.random.randint(0,len(keys))\n",
    "        selected_key = keys[rand_idx]\n",
    "        return selected_key\n",
    "\n",
    "def create_triplets(d2v_model, article_names, article_weights, nb_triplets=20, triplets_per_file=5, neg_ratio=0.5, str_mode = False) :\n",
    "    \"\"\"\n",
    "    inputs :    \n",
    "        - d2v_model : paragraph vector model \n",
    "        - article_names : ndarray containing the names of the json files (absolute path !)\n",
    "        - article_weights: ndarray normalized of the weight of each files \n",
    "        - nb_triplets : nb of triplets to generate\n",
    "        - triplets_per_file : number of triplet built for each selected file\n",
    "        - neg_ratio : ratio of positives / negative examples. Negative examples are taken inside the article !\n",
    "        \n",
    "    output : \n",
    "        - triplets : nd_array of triplets of shape (nb_triplets+ , embed_dim)\n",
    "        - labels : nd_array of labels of shape (nb_triplets+ ,)\n",
    "\n",
    "    \"\"\"\n",
    "    triplets = []\n",
    "    labels = []\n",
    "    \n",
    "    assert nb_triplets>=triplets_per_file, \"you should have nb_triplets > triplets_per_file\"\n",
    "    \n",
    "    # nb of pos / neg triplets per file\n",
    "    neg_per_file = np.floor(triplets_per_file*neg_ratio) #number of negative triplets to generate given(query + partial summary)\n",
    "    assert neg_per_file >= 1, \"you have to increase your neg_ratio\"\n",
    "    \n",
    "    nb_files = nb_triplets / triplets_per_file\n",
    "    selected_files_array = np.random.choice(article_names, size=nb_files, p=article_weights, replace = False)\n",
    "    \n",
    "    for full_name in selected_files_array :\n",
    "        with open(full_name) as f :\n",
    "            file_as_dict = json.load(f)\n",
    "        \n",
    "        counter = 0\n",
    "        while counter < triplets_per_file :\n",
    "            \n",
    "            # select a key for positive examples\n",
    "            key_pos = select_key(file_as_dict)\n",
    "            \n",
    "            triplet = build_triplet(d2v_model, file_as_dict, key_pos, positive = True, str_mode = str_mode)\n",
    "            label = 1\n",
    "            \n",
    "            triplets.append(triplet)\n",
    "            labels.append(label)\n",
    "            counter += 1 \n",
    "            \n",
    "            if neg_ratio < 1 : \n",
    "                \n",
    "                if np.random.rand() < neg_ratio :\n",
    "                    \n",
    "                    triplet = build_triplet(d2v_model, file_as_dict, key_pos, positive = False, str_mode = str_mode)\n",
    "                    label = 0\n",
    "                    \n",
    "                    triplets.append(triplet)\n",
    "                    labels.append(label)\n",
    "                    counter += 1 \n",
    "\n",
    "            else :\n",
    "                \n",
    "                for n in range(int(np.floor(neg_ratio))):\n",
    "                    \n",
    "                    triplet = build_triplet(d2v_model, file_as_dict, key_pos, positive = False, str_mode = str_mode)\n",
    "                    label = 0\n",
    "                    \n",
    "                    triplets.append(triplet)\n",
    "                    labels.append(label)\n",
    "                    counter += 1 \n",
    "\n",
    "            \n",
    "    triplets = np.asarray(triplets)[:nb_triplets]\n",
    "    labels = np.asarray(labels)[:nb_triplets]\n",
    "    \n",
    "    return triplets, labels\n",
    "\n",
    "def build_triplet(d2v_model, file_as_dict, key_pos, positive = True, str_mode = False):\n",
    "\n",
    "    query_str = key_pos\n",
    "    query_prep = gensim.utils.simple_preprocess(query_str, deacc=True)\n",
    "    query_vector = d2v_model.infer_vector(query_prep)\n",
    "    \n",
    "    summary_str = file_as_dict[key_pos]\n",
    "    sentences = summary_str.split(\".\")\n",
    "    \n",
    "    partial_summary = []\n",
    "    candidates = []\n",
    "    \n",
    "    size_partial_summary = np.random.rand()\n",
    "    \n",
    "    for sentence in sentences: \n",
    "        if np.random.rand() < size_partial_summary :\n",
    "            partial_summary.append(sentence)\n",
    "        else :\n",
    "            candidates.append(sentence)\n",
    "    \n",
    "    candidate = \"\"\n",
    "    counter_candidate = 0\n",
    "    while (candidate == \"\" or partial_summary == \"\") and counter_candidate < 10:\n",
    "        counter_candidate += 1\n",
    "        \n",
    "        if positive : \n",
    "            if len(candidates) > 0:\n",
    "                random_candidate_index = np.random.randint(0,len(candidates))\n",
    "                candidate = candidates[random_candidate_index]\n",
    "            else :\n",
    "                random_candidate_index = np.random.randint(0,len(partial_summary))\n",
    "                candidate = partial_summary[random_candidate_index]\n",
    "                partial_summary[random_candidate_index] = \"\"\n",
    "\n",
    "\n",
    "            candidate_prep = gensim.utils.simple_preprocess(candidate, deacc=True)\n",
    "            candidate_vector = d2v_model.infer_vector(candidate_prep)\n",
    "\n",
    "        else :\n",
    "\n",
    "            key_neg = select_key(file_as_dict)\n",
    "            counter = 0\n",
    "\n",
    "            while key_neg == key_pos and counter<10 : # the counter is for the preproduction code \n",
    "                counter += 1\n",
    "                key_neg = select_key(file_as_dict)\n",
    "\n",
    "            summary_str = file_as_dict[key_neg]\n",
    "\n",
    "            sentences = summary_str.split('.')\n",
    "            random_candidate_index = np.random.randint(0,len(sentences))\n",
    "            candidate = sentences[random_candidate_index]\n",
    "            candidate_prep = gensim.utils.simple_preprocess(candidate, deacc=True)\n",
    "            candidate_vector = d2v_model.infer_vector(candidate_prep)\n",
    "        \n",
    "        partial_summary_str = \"\".join(partial_summary)\n",
    "        partial_summary_prep = gensim.utils.simple_preprocess(partial_summary_str, deacc=True)\n",
    "        partial_summary_vector = d2v_model.infer_vector(partial_summary_prep)\n",
    "    \n",
    "    if str_mode :\n",
    "        return query_str, partial_summary_str, candidate\n",
    "    else :\n",
    "        return np.hstack( [query_vector, partial_summary_vector, candidate_vector] )\n",
    "\n",
    "\n",
    "def doc_title_table(title_file):\n",
    "    with open(title_file , 'r') as f :\n",
    "        lines = f.readlines()\n",
    "        raw_text = \"\".join(l for l in lines)\n",
    "        left_idx_num = [ m.end(0) for m in re.finditer(r\"<num>\",raw_text)]\n",
    "        right_idx_num = [ m.start(0) for m in re.finditer(r\"</num>\",raw_text)]\n",
    "\n",
    "        left_idx_title = [ m.end(0) for m in re.finditer(r\"<title>\",raw_text)]\n",
    "        right_idx_title = [ m.start(0) for m in re.finditer(r\"</title>\",raw_text)]\n",
    "\n",
    "        docs_title_dict = {}\n",
    "        for i in range(len(left_idx_num)):\n",
    "            docs_title_dict[raw_text[left_idx_num[i]+1:right_idx_num[i]-1]] = raw_text[left_idx_title[i]+1:right_idx_title[i]-1]\n",
    "    return docs_title_dict\n",
    "\n",
    "def merge_articles(docs_folder):\n",
    "\n",
    "    s = \"\"\n",
    "    \n",
    "    for doc in os.listdir(docs_folder):\n",
    "        try:\n",
    "            with open(docs_folder + doc ,'r') as f:\n",
    "\n",
    "                lines = f.readlines()\n",
    "                raw_doc = \"\".join(txt for txt in lines)\n",
    "                left_idx_headline = [ m.end(0) for m in re.finditer(r\"<HEADLINE>\",raw_doc)]\n",
    "                right_idx_headline = [ m.start(0) for m in re.finditer(r\"</HEADLINE>\",raw_doc)]\n",
    "\n",
    "                left_idx_text = [ m.end(0) for m in re.finditer(r\"<TEXT>\",raw_doc)]\n",
    "                right_idx_text = [ m.start(0) for m in re.finditer(r\"</TEXT>\",raw_doc)]\n",
    "\n",
    "                raw_headline = raw_doc[left_idx_headline[0]:right_idx_headline[0]]\n",
    "                raw_text = raw_doc[left_idx_text[0]:right_idx_text[0]]\n",
    "\n",
    "                left_idx_paragraph_headline = [ m.end(0) for m in re.finditer(r\"<P>\",raw_headline)]\n",
    "                right_idx_paragraph_headline = [ m.start(0) for m in re.finditer(r\"</P>\",raw_headline)]\n",
    "\n",
    "                left_idx_paragraph_text = [ m.end(0) for m in re.finditer(r\"<P>\",raw_text)]\n",
    "                right_idx_paragraph_text = [ m.start(0) for m in re.finditer(r\"</P>\",raw_text)]\n",
    "\n",
    "                for i in range(len(left_idx_paragraph_headline)):\n",
    "                    s += raw_headline[left_idx_paragraph_headline[i]:right_idx_paragraph_headline[i]-2] + \".\"\n",
    "\n",
    "                for i in range(len(left_idx_paragraph_text)):\n",
    "                    s += raw_text[left_idx_paragraph_text[i]:right_idx_paragraph_text[i]-1]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return s\n",
    "\n",
    "def summarize(text, query, d2v_model, nn_model, limit = 250):\n",
    "\n",
    "    query_prep = gensim.utils.simple_preprocess(query, deacc=True)\n",
    "    query_vector = d2v_model.infer_vector(query_prep)\n",
    "    \n",
    "    summary  = \"\"\n",
    "    summary_vector = d2v_model.infer_vector([\"\"])\n",
    "    summary_idx = []\n",
    "    \n",
    "    sentences = text.split('.')\n",
    "    sentences = np.asarray(sentences)\n",
    "    \n",
    "    remaining_sentences = copy.copy(sentences)\n",
    "    \n",
    "    size = 0\n",
    "    counter = 0\n",
    "    while size < limit and len(remaining_sentences)>0 :\n",
    "        counter = counter+1\n",
    "        scores = []\n",
    "        for sentence in remaining_sentences :\n",
    "            \n",
    "            \n",
    "            sentence_prep = gensim.utils.simple_preprocess(sentence, deacc=True)\n",
    "            sentence_vector = d2v_model.infer_vector(sentence_prep)\n",
    "\n",
    "            nn_input = np.hstack([query_vector, summary_vector, sentence_vector])\n",
    "            nn_input = np.asarray([nn_input]) # weird but it is important to do it\n",
    "            score = nn_model.predict(nn_input) \n",
    "            scores.append(score)\n",
    "        #print(scores)\n",
    "        max_idx_rem = int(np.argmax(scores))\n",
    "        idx_selected_sentence = np.arange(len(sentences))[sentences == remaining_sentences[max_idx_rem]]\n",
    "        idx_selected_sentence = int(idx_selected_sentence[0])\n",
    "        size += len(remaining_sentences[max_idx_rem].split())\n",
    "        \n",
    "        remaining_sentences = list(remaining_sentences)\n",
    "        del remaining_sentences[max_idx_rem]\n",
    "        bisect.insort_left(summary_idx,idx_selected_sentence)\n",
    "\n",
    "        summary  = \"\"\n",
    "\n",
    "        for idx in summary_idx:\n",
    "            summary = summary + \" \" + sentences[idx]\n",
    "\n",
    "        summary_prep = gensim.utils.simple_preprocess(summary, deacc=True)\n",
    "        summary_vector = d2v_model.infer_vector(summary_prep)\n",
    "\n",
    "    return summary\n",
    "\n",
    "## loading a d2vmodel (to be a shifted LSTM next ...)\n",
    "\n",
    "# parameters of doc2vec\n",
    "dm = 0\n",
    "min_count = 5\n",
    "window = 10\n",
    "size = 400\n",
    "sample = 1e-4\n",
    "negative = 5\n",
    "workers = 4\n",
    "epoch = 20\n",
    "\n",
    "# Initialize the model ( IMPORTANT )\n",
    "d2v_model = gensim.models.doc2vec.Doc2Vec(dm=dm,min_count=min_count, window=window, size=size, sample=sample, negative=negative, workers=workers,iter = epoch)\n",
    "\n",
    "# load model\n",
    "model_name =\"dm_\"+str(dm)+\"_mc_\"+str(min_count)+\"_w_\"+str(window)+\"_size_\"+str(size)+\"_neg_\"+str(negative)+\"_ep_\"+str(epoch)\n",
    "d2v_model = d2v_model.load(model_folder+model_name+\".d2v\")\n",
    "print(\"model loaded\")\n",
    "\n",
    "\n",
    "## get wikipedia data\n",
    "\n",
    "article_names, article_weights = relevant_articles(data_json)\n",
    "\n",
    "# DUC data \n",
    "docs_title_dict = doc_title_table(title_file)\n",
    "\n",
    "## design a fully connected model\n",
    "\n",
    "fc_model_name = nn_models_folder + time.strftime(\"%Y_%m_%d_\") +'_fc_model.h5' # replace it with hour of training\n",
    "\n",
    "fc_model = Sequential()\n",
    "\n",
    "fc_model.add(Dense(120, input_dim=1200))\n",
    "fc_model.add(Activation('sigmoid'))\n",
    "fc_model.add(Dropout(0.5))\n",
    "\n",
    "fc_model.add(Dense(12))\n",
    "fc_model.add(Activation('sigmoid'))\n",
    "fc_model.add(Dropout(0.5))\n",
    "\n",
    "fc_model.add(Dense(1))\n",
    "fc_model.add(Activation('sigmoid'))\n",
    "\n",
    "# compiling the model\n",
    "fc_model.compile(loss=\"binary_crossentropy\", optimizer='sgd')\n",
    "\n",
    "# training per batch\n",
    "\n",
    "batch_size = 128\n",
    "patience = 0\n",
    "valLoss_min = 1\n",
    "batch_counter = 0\n",
    "while patience < patience_limit :\n",
    "    # train on 1000 batchs\n",
    "    for i in range(1000):\n",
    "        \n",
    "        triplets, labels = create_triplets(d2v_model, article_names, article_weights, nb_triplets=batch_size, triplets_per_file=16, neg_ratio=1, str_mode = False)\n",
    "        fc_model.train_on_batch(triplets, labels)\n",
    "    \n",
    "    batch_counter += 1\n",
    "    # check if the model improved \n",
    "    \n",
    "    valLoss =  fc_model.evaluate(X_val,y_val, batch_size = 100)\n",
    "    \n",
    "    if valLoss < valLoss_min :\n",
    "        patience = 0\n",
    "        #save this new model\n",
    "        fc_model_name = \"fc_model_batch_\"+str(batch_counter)+\"k_valLoss_\"+str(valLoss)\n",
    "        fc_model.save(nn_models_folder + fc_model_name+ \".h5\")  # creates a HDF5 file 'my_model.h5'\n",
    "\n",
    "        # summarize DUC\n",
    "        system_folder = \"/home/ubuntu/summarization_query_oriented/DUC/duc2005_summary_system/\"+fc_model_name+\"/\"\n",
    "        os.mkdir(system_folder)\n",
    "        for docs_key in docs_title_dict.keys():\n",
    "\n",
    "            docs_folder = titles_folder+docs_key+\"/\"\n",
    "            text = merge_articles(docs_folder)\n",
    "            query = docs_title_dict[docs_key]\n",
    "            summary = summarize(text,query,d2v_model, fc_model, limit = 250)\n",
    "\n",
    "            summary = \" \".join(summary.split()[:250])\n",
    "\n",
    "            with open(system_folder+docs_key,'w') as f :\n",
    "                f.write(summary)\n",
    "                print 'writing in '+ system_folder + docs_key\n",
    "\n",
    "            \n",
    "        # perform rouge\n",
    "        r = Rouge155()\n",
    "        r.system_dir = system_folder\n",
    "        r.model_dir = '/home/ubuntu/summarization_query_oriented/DUC/duc2005_summary_model'\n",
    "        r.system_filename_pattern = 'd(\\d+)[a-z]'\n",
    "        r.model_filename_pattern = 'D#ID#.M.250.[A-Z].[A-Z]'\n",
    "\n",
    "        output = r.convert_and_evaluate()\n",
    "        print 50*'$'\n",
    "        print (fc_model_name)\n",
    "        print(output)\n",
    "        print 50*'$'\n",
    "        \n",
    "        # save rouge results\n",
    "        output_dict = r.output_to_dict(output)\n",
    "        with open(system_folder+\"ROUGE_RESULTS.json\",'w') as f :\n",
    "            json.dump(output_dict,f)\n",
    "        \n",
    "    else :\n",
    "        patience = patience + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
