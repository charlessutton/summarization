{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# training file , I remove the concept of validation loss\n",
    "\n",
    "# In this script we perform the training of the fully connected model\n",
    "\n",
    "# Import \n",
    "import gensim\n",
    "import keras\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "import pyrouge\n",
    "from pyrouge import Rouge155\n",
    "\n",
    "from hard_coded import data_json_dir, data_txt_dir, lang_model_dir, model_dir, nn_summarizers_dir, summary_system_super_dir, tdqfs_folder\n",
    "from hard_coded import non_selected_keys,tdqfs_themes\n",
    "from functions.training_functions import *\n",
    "\n",
    "# paths to folder \n",
    "data_json = data_json_dir\n",
    "data_txt = data_txt_dir\n",
    "lang_model_folder = lang_model_dir\n",
    "nn_summarizers_folder = nn_summarizers_dir\n",
    "summary_system_super_folder = summary_system_super_dir\n",
    "themes = tdqfs_themes\n",
    "\n",
    "#title_file = \"/home/ubuntu/summarization_query_oriented/data/DUC/duc2005_topics.sgml\"\n",
    "#titles_folder = \"/home/ubuntu/summarization_query_oriented/data/DUC/duc2005_docs/\"\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "patience_limit = 25\n",
    "\n",
    "## loading a d2vmodel (to be a shifted LSTM next ...)\n",
    "\n",
    "# parameters of doc2vec\n",
    "dm = 0\n",
    "min_count = 5\n",
    "window = 10\n",
    "size = 400\n",
    "sample = 1e-4\n",
    "negative = 5\n",
    "workers = 4\n",
    "epoch = 20\n",
    "\n",
    "# Initialize the model ( IMPORTANT )\n",
    "d2v_model = gensim.models.doc2vec.Doc2Vec(dm=dm,min_count=min_count, window=window, size=size, sample=sample, negative=negative, workers=workers,iter = epoch)\n",
    "\n",
    "# load model\n",
    "model_name =\"dm_\"+str(dm)+\"_mc_\"+str(min_count)+\"_w_\"+str(window)+\"_size_\"+str(size)+\"_neg_\"+str(negative)+\"_ep_\"+str(epoch)\n",
    "try :\n",
    "    d2v_model = d2v_model.load(lang_model_folder+model_name+\".d2v\")\n",
    "except :\n",
    "    print \"try a model in : \", os.listdir(lang_model_folder)\n",
    "print(\"model loaded\")\n",
    "\n",
    "## get wikipedia data\n",
    "article_names, article_weights = relevant_articles(data_json)\n",
    "\n",
    "## design a fully connected model\n",
    "\n",
    "fc_model = Sequential()\n",
    "\n",
    "fc_model.add(Dense(120, input_dim=1200))\n",
    "fc_model.add(Activation('sigmoid'))\n",
    "fc_model.add(Dropout(0.5))\n",
    "\n",
    "fc_model.add(Dense(12))\n",
    "fc_model.add(Activation('sigmoid'))\n",
    "fc_model.add(Dropout(0.5))\n",
    "\n",
    "fc_model.add(Dense(1))\n",
    "fc_model.add(Activation('sigmoid'))\n",
    "\n",
    "# compiling the model\n",
    "fc_model.compile(loss=\"binary_crossentropy\", optimizer='sgd')\n",
    "\n",
    "# training per batch\n",
    "batch_per_epoch = 2000\n",
    "#batch_per_epoch = 20\n",
    "batch_size = 128\n",
    "patience = 0\n",
    "batch_counter = 19\n",
    "rouge_su4_recall_max = 0\n",
    "rouge_2_recall_max = 0\n",
    "\n",
    "while patience < patience_limit :\n",
    "    # train on several batchs\n",
    "    for i in range(batch_per_epoch):\n",
    "        triplets, labels = create_triplets(d2v_model, article_names, article_weights, nb_triplets=batch_size, triplets_per_file=16, neg_ratio=1, str_mode = False)\n",
    "        fc_model.train_on_batch(triplets, labels)\n",
    "    \n",
    "    batch_counter += 1\n",
    "\n",
    "    # summarize DUC\n",
    "    str_time = time.strftime(\"%Y_%m_%d\")\n",
    "    fc_model_name = str_time+\"_fc_model_batch_\"+str(batch_counter)+\"k\"\n",
    "    system_folder = summary_system_super_folder+fc_model_name+\"/\"\n",
    "    os.mkdir(system_folder)\n",
    "\n",
    "    for theme in themes :\n",
    "        theme_folder = tdqfs_folder + theme + \"/\"\n",
    "        theme_doc_folder = theme_folder + theme + \"/\"\n",
    "        queries = get_queries(theme_folder+\"queries.txt\")\n",
    "        text = merge_articles_tqdfs(theme_doc_folder)\n",
    "        for i in range(len(queries)):\n",
    "            query = queries[i]\n",
    "            summary = summarize(text,query,d2v_model, fc_model, limit = 250)\n",
    "            summary = \" \".join(summary.split()[:250])\n",
    "\n",
    "            summary_name = theme + \".\" + str(i+1) + \".txt\"\n",
    "            with open(system_folder + summary_name,'w') as f :\n",
    "                f.write(summary.decode('ascii',\"ignore\").encode(\"utf8\", \"replace\"))\n",
    "                print 'writing in '+ system_folder + summary_name\n",
    "\n",
    "    r = Rouge155()\n",
    "    r.system_dir = system_folder\n",
    "    r.model_dir = model_dir\n",
    "    r.model_filename_pattern = '#ID#.u[0-9]q[0-9].txt'\n",
    "    r.system_filename_pattern = '([a-z]+.[0-9]+).txt'            \n",
    "\n",
    "    options = \"-e \" + r._data_dir + \" -n 4 -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0 -a -x\"\n",
    "\n",
    "    output = r.convert_and_evaluate(rouge_args=options)\n",
    "    output_dict = r.output_to_dict(output)\n",
    "\n",
    "    rouge_2_recall = np.round( output_dict[\"rouge_2_recall\"], 5 )\n",
    "    rouge_su4_recall = np.round( output_dict[\"rouge_su4_recall\"], 5 )\n",
    "\n",
    "    print 50*'$'\n",
    "    print \"rouge_2\", rouge_2_recall\n",
    "    print \"rouge_SU4\", rouge_su4_recall\n",
    "    print 50*'$'\n",
    "\n",
    "    # save rouge results\n",
    "    output_dict = r.output_to_dict(output)\n",
    "    with open(system_folder+\"ROUGE_RESULTS.json\",'w') as f :\n",
    "        json.dump(output_dict,f)\n",
    "    #check if the model has improved \n",
    "    if rouge_2_recall > rouge_2_recall_max or rouge_su4_recall > rouge_su4_recall_max :\n",
    "        patience = 0\n",
    "        if rouge_2_recall > rouge_2_recall_max :\n",
    "            rouge_2_recall_max = rouge_2_recall\n",
    "        if rouge_su4_recall > rouge_su4_recall_max :\n",
    "            rouge_su4_recall_max = rouge_su4_recall    \n",
    "            #save this new model\n",
    "        fc_model_name =\"fc_model_batch_\"+str(batch_counter)+\"k_R2_\"+str(rouge_2_recall)+\"_SU4_\"+str(rouge_su4_recall)\n",
    "        fc_model.save(nn_summarizers_folder + fc_model_name + \".hdf5\")  # creates a HDF5 file 'my_model.h5'\n",
    "        print fc_model_name, \"is saved\"\n",
    "    else :\n",
    "        patience = patience + 1\n",
    "        print \"patience :\", patience\n",
    "    \n",
    "    \n",
    "print('early stopped')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
